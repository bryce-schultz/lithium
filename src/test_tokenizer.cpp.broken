#define CTEST

#include <string>
#include <sstream>
#include <fstream>

#include "ctest.h"
#include "Tokenizer.h"
#include "Token.h"

Test test_tokenizer_lex_single_character_token()
{
    std::istringstream input(";");
    Tokenizer tokenizer(input);
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), ';');
    assertStringEqual(token.getValue().c_str(), ";");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test test_tokenizer_lex_two_character_token()
{
    std::istringstream input("==");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::EQ);
    assertStringEqual(token.getValue().c_str(), "==");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test test_tokenizer_lex_identifier()
{
    std::istringstream input("myVariable");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::IDENT);
    assertStringEqual(token.getValue().c_str(), "myVariable");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test test_tokenizer_lex_number()
{
    std::istringstream input("12345");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    assertEqual(token.getType(), Token::NUMBER);
    assertStringEqual(token.getValue().c_str(), "12345");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_float_number()
{
    std::istringstream input("123.45");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    assertEqual(token.getType(), Token::NUMBER);
    assertStringEqual(token.getValue().c_str(), "123.45");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_float_number_invalid()
{
    std::istringstream input("123.45.67");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    assertEqual(token.getType(), Token::NUMBER);
    assertStringEqual(token.getValue().c_str(), "123.45");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
    token = tokenizer.lex(); // consume the . token
    assertEqual(token.getType(), '.');
    assertStringEqual(token.getValue().c_str(), ".");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 7);
    token = tokenizer.lex(); // consume the next character
    assertEqual(token.getType(), Token::NUMBER);
    assertStringEqual(token.getValue().c_str(), "67");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 8);
    token = tokenizer.lex(); // consume the end
    assertEqual(token.getType(), Token::END);
    assertStringEqual(token.getValue().c_str(), "");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 10);
}

Test tokenizer_read_from_file()
{
    std::fstream file("test.txt", std::ios::in | std::ios::out | std::ios::trunc);
    if (!file.is_open()) {
        throw std::runtime_error("Failed to open test.txt");
    }

    // empty the file and write a test case
    file.clear();
    file.seekg(0, std::ios::end);
    file << "123.45.67\n";
    file.flush();
    file.seekg(0, std::ios::beg);
    
    Tokenizer tokenizer(file, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::NUMBER);
    assertStringEqual(token.getValue().c_str(), "123.45");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
    token = tokenizer.lex(); // consume the . token
    assertEqual(token.getType(), '.');
    assertStringEqual(token.getValue().c_str(), ".");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 7);
    token = tokenizer.lex(); // consume the next character
    assertEqual(token.getType(), Token::NUMBER);
    assertStringEqual(token.getValue().c_str(), "67");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 8);
    token = tokenizer.lex(); // consume the end
    assertEqual(token.getType(), Token::END);
    assertStringEqual(token.getValue().c_str(), "");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 10);
    token = tokenizer.lex(); // check for end of input
    assertEqual(token.getType(), Token::END);
    assertStringEqual(token.getValue().c_str(), "");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 10);
    
    file.close();

    // Clean up the test file
    std::remove("test.txt");
}

Test tokenizer_lex_string()
{
    std::istringstream input("\"Hello, World!\"");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::STRING);
    assertStringEqual(token.getValue().c_str(), "Hello, World!");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
    assertEqual(token.getRange().getEnd().getLine(), 1);
    assertEqual(token.getRange().getEnd().getColumn(), 16);
}

Test tokenizer_lex_string_invalid()
{
    std::istringstream input("\"Hello, World!");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::JUNK);
    assertStringEqual(token.getValue().c_str(), "Hello, World!");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
    assertEqual(token.getRange().getEnd().getLine(), 1);
    assertEqual(token.getRange().getEnd().getColumn(), 15);
}

Test tokenizer_lex_keyword_let()
{
    std::istringstream input("let");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::LET);
    assertStringEqual(token.getValue().c_str(), "let");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_keyword_const()
{
    std::istringstream input("const");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::CONST);
    assertStringEqual(token.getValue().c_str(), "const");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_keyword_if()
{
    std::istringstream input("if");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::IF);
    assertStringEqual(token.getValue().c_str(), "if");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_keyword_else()
{
    std::istringstream input("else");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::ELSE);
    assertStringEqual(token.getValue().c_str(), "else");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_keyword_for()
{
    std::istringstream input("for");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::FOR);
    assertStringEqual(token.getValue().c_str(), "for");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_keyword_while()
{
    std::istringstream input("while");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::WHILE);
    assertStringEqual(token.getValue().c_str(), "while");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_keyword_fn()
{
    std::istringstream input("fn");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::FN);
    assertStringEqual(token.getValue().c_str(), "fn");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_keyword_return()
{
    std::istringstream input("return");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::RETURN);
    assertStringEqual(token.getValue().c_str(), "return");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_keyword_class()
{
    std::istringstream input("class");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::CLASS);
    assertStringEqual(token.getValue().c_str(), "class");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_keyword_import()
{
    std::istringstream input("import");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::IMPORT);
    assertStringEqual(token.getValue().c_str(), "import");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_keyword_print()
{
    std::istringstream input("print");
    Tokenizer tokenizer(input, "test.txt");
    Token token = tokenizer.lex();
    
    assertEqual(token.getType(), Token::PRINT);
    assertStringEqual(token.getValue().c_str(), "print");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);
}

Test tokenizer_lex_all_single_character_tokens()
{
    std::istringstream input("+-*/%;,(){}[].");
    Tokenizer tokenizer(input);

    Token token = tokenizer.lex();
    assertEqual(token.getType(), '+');
    assertStringEqual(token.getValue().c_str(), "+");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);

    token = tokenizer.lex();
    assertEqual(token.getType(), '-');
    assertStringEqual(token.getValue().c_str(), "-");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 2);
    
    token = tokenizer.lex();
    assertEqual(token.getType(), '*');
    assertStringEqual(token.getValue().c_str(), "*");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 3);
    
    token = tokenizer.lex();
    assertEqual(token.getType(), '/');
    assertStringEqual(token.getValue().c_str(), "/");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 4);
    
    token = tokenizer.lex();
    assertEqual(token.getType(), '%');
    assertStringEqual(token.getValue().c_str(), "%");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 5);
    
    token = tokenizer.lex();
    assertEqual(token.getType(), ';');
    assertStringEqual(token.getValue().c_str(), ";");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 6);
    
    token = tokenizer.lex();
    assertEqual(token.getType(), ',');
    assertStringEqual(token.getValue().c_str(), ",");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 7);

    token = tokenizer.lex();
    assertEqual(token.getType(), '(');
    assertStringEqual(token.getValue().c_str(), "(");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 8);
    
    token = tokenizer.lex();
    assertEqual(token.getType(), ')');
    assertStringEqual(token.getValue().c_str(), ")");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 9);

    token = tokenizer.lex();
    assertEqual(token.getType(), '{');
    assertStringEqual(token.getValue().c_str(), "{");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 10);

    token = tokenizer.lex();
    assertEqual(token.getType(), '}');
    assertStringEqual(token.getValue().c_str(), "}");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 11);

    token = tokenizer.lex();
    assertEqual(token.getType(), '[');
    assertStringEqual(token.getValue().c_str(), "[");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 12);

    token = tokenizer.lex();
    assertEqual(token.getType(), ']');
    assertStringEqual(token.getValue().c_str(), "]");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 13);

    token = tokenizer.lex();
    assertEqual(token.getType(), '.');
    assertStringEqual(token.getValue().c_str(), ".");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 14);

    token = tokenizer.lex(); // consume the end
    assertEqual(token.getType(), Token::END);
    assertStringEqual(token.getValue().c_str(), "");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 15);
}

Test tokenizer_lex_all_two_character_tokens()
{
    std::istringstream input("== != <= >= && || ++ --");
    Tokenizer tokenizer(input);

    Token token = tokenizer.lex();
    assertEqual(token.getType(), Token::EQ);
    assertStringEqual(token.getValue().c_str(), "==");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 1);

    token = tokenizer.lex();
    assertEqual(token.getType(), Token::NE);
    assertStringEqual(token.getValue().c_str(), "!=");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 4);

    token = tokenizer.lex();
    assertEqual(token.getType(), Token::LE);
    assertStringEqual(token.getValue().c_str(), "<=");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 7);

    token = tokenizer.lex();
    assertEqual(token.getType(), Token::GE);
    assertStringEqual(token.getValue().c_str(), ">=");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 10);

    token = tokenizer.lex();
    assertEqual(token.getType(), Token::AND);
    assertStringEqual(token.getValue().c_str(), "&&");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 13);

    token = tokenizer.lex();
    assertEqual(token.getType(), Token::OR);
    assertStringEqual(token.getValue().c_str(), "||");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 16);

    token = tokenizer.lex();
    assertEqual(token.getType(), Token::INC);
    assertStringEqual(token.getValue().c_str(), "++");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 19);

    token = tokenizer.lex();
    assertEqual(token.getType(), Token::DEC);
    assertStringEqual(token.getValue().c_str(), "--");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 22);
    token = tokenizer.lex(); // consume the end
    assertEqual(token.getType(), Token::END);
    assertStringEqual(token.getValue().c_str(), "");
    assertEqual(token.getRange().getStart().getLine(), 1);
    assertEqual(token.getRange().getStart().getColumn(), 24);
}

int main()
{
    START_TEST;

    START_GROUP(Tokenizer Tests);
    TEST(test_tokenizer_lex_two_character_token);
    TEST(test_tokenizer_lex_identifier);
    TEST(test_tokenizer_lex_number);
    TEST(tokenizer_lex_float_number);
    TEST(tokenizer_lex_float_number_invalid);
    TEST(tokenizer_read_from_file);
    TEST(tokenizer_lex_string);
    TEST(tokenizer_lex_string_invalid);
    END_GROUP;

    START_GROUP(Single Character Token Tests);
    TEST(test_tokenizer_lex_single_character_token);
    TEST(tokenizer_lex_all_single_character_tokens);
    END_GROUP;

    START_GROUP(Two Character Token Tests);
    TEST(tokenizer_lex_all_two_character_tokens);
    END_GROUP;

    START_GROUP(Keyword Tests);
    TEST(tokenizer_lex_keyword_let);
    TEST(tokenizer_lex_keyword_const);
    TEST(tokenizer_lex_keyword_if);
    TEST(tokenizer_lex_keyword_else);
    TEST(tokenizer_lex_keyword_for);
    TEST(tokenizer_lex_keyword_while);
    TEST(tokenizer_lex_keyword_fn);
    TEST(tokenizer_lex_keyword_return);
    TEST(tokenizer_lex_keyword_class);
    TEST(tokenizer_lex_keyword_import);
    TEST(tokenizer_lex_keyword_print);
    END_GROUP;

    END_TEST;
}